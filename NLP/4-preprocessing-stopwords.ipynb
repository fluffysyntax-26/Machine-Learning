{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ba42483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b50ef45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = '''Machine learning is evolving fast. Models learn from data. Engineers clean the text carefully. They remove stopwords first. They normalize the words next. Some words are playing. Others were played earlier. People study models daily. They studied them yesterday. Systems are running now. Some systems ran before. Accuracy improves slowly. Better features help models. Good preprocessing matters a lot.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6b6d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bb8af3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords defined in nltk\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b39def1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine learning is evolving fast.',\n",
       " 'Models learn from data.',\n",
       " 'Engineers clean the text carefully.',\n",
       " 'They remove stopwords first.',\n",
       " 'They normalize the words next.',\n",
       " 'Some words are playing.',\n",
       " 'Others were played earlier.',\n",
       " 'People study models daily.',\n",
       " 'They studied them yesterday.',\n",
       " 'Systems are running now.',\n",
       " 'Some systems ran before.',\n",
       " 'Accuracy improves slowly.',\n",
       " 'Better features help models.',\n",
       " 'Good preprocessing matters a lot.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize words, apply stemming/lemmatisation to those words and remove them using stopwords\n",
    "sentences = sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb5bd9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machin learn evolv fast .',\n",
       " 'model learn data .',\n",
       " 'engin clean text care .',\n",
       " 'they remov stopword first .',\n",
       " 'they normal word next .',\n",
       " 'some word play .',\n",
       " 'other play earlier .',\n",
       " 'peopl studi model daili .',\n",
       " 'they studi yesterday .',\n",
       " 'system run .',\n",
       " 'some system ran .',\n",
       " 'accuraci improv slowli .',\n",
       " 'better featur help model .',\n",
       " 'good preprocess matter lot .']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize each word, apply stemming and join it back again to a sentence\n",
    "processed = []\n",
    "for sentence in sentences: \n",
    "    words = word_tokenize(sentence)\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    processed.append(\" \".join(words))\n",
    "\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efcdf0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Machine learning is evolving fast.',\n",
       "  'Models learn from data.',\n",
       "  'Engineers clean the text carefully.',\n",
       "  'They remove stopwords first.',\n",
       "  'They normalize the words next.',\n",
       "  'Some words are playing.',\n",
       "  'Others were played earlier.',\n",
       "  'People study models daily.',\n",
       "  'They studied them yesterday.',\n",
       "  'Systems are running now.',\n",
       "  'Some systems ran before.',\n",
       "  'Accuracy improves slowly.',\n",
       "  'Better features help models.',\n",
       "  'Good preprocessing matters a lot.'],\n",
       " ['machin learn evolv fast .',\n",
       "  'model learn data .',\n",
       "  'engin clean text care .',\n",
       "  'they remov stopword first .',\n",
       "  'they normal word next .',\n",
       "  'some word play .',\n",
       "  'other play earlier .',\n",
       "  'peopl studi model daili .',\n",
       "  'they studi yesterday .',\n",
       "  'system run .',\n",
       "  'some system ran .',\n",
       "  'accuraci improv slowli .',\n",
       "  'better featur help model .',\n",
       "  'good preprocess matter lot .'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the same as above using Snowball stemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "snowballs = []\n",
    "for sentence in sentences: \n",
    "    words = word_tokenize(sentence)\n",
    "    words = [snowball_stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    snowballs.append(' '.join(words))\n",
    "\n",
    "sentences, snowballs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "053d8541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine learning evolving fast .',\n",
       " 'Models learn data .',\n",
       " 'Engineers clean text carefully .',\n",
       " 'They remove stopwords first .',\n",
       " 'They normalize word next .',\n",
       " 'Some word playing .',\n",
       " 'Others played earlier .',\n",
       " 'People study model daily .',\n",
       " 'They studied yesterday .',\n",
       " 'Systems running .',\n",
       " 'Some system ran .',\n",
       " 'Accuracy improves slowly .',\n",
       " 'Better feature help model .',\n",
       " 'Good preprocessing matter lot .']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the same using lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "processed = []\n",
    "\n",
    "for sentence in sentences: \n",
    "    words = word_tokenize(sentence)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    processed.append(\" \".join(words))\n",
    "\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f950e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
